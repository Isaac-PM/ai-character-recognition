{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN/RIKMXVmXrevB/gmueDYf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Isaac-PM/ai-character-recognition/blob/main/testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reconocimiento de dígitos (números)\n",
        "\n",
        "## Generalidades\n",
        "\n",
        "- Se usan imágenes de 28x28 píxeles, siendo en total 784 píxeles.\n",
        "- Cada píxel tiene un valor que se encuentra en un rango de 0 a 255, siendo **0 completamente negro** y **255 completamente blanco**.\n",
        "- Red neuronal<sup>1</sup> de dos capas<sup>2</sup>, sin contar a la capa de entrada como una.\n",
        "- El conjunto de datos a utilizar es **MNIST database**.\n",
        "- La idea central es, que durante el proceso de entrenamiento de la red, los parámetros de esta se ajusten para aproximarse a una función deseada (reconocer dígitos).\n",
        "\n",
        "<center><img   src=\"https://i.imgur.com/tTH00qo.jpg\"   width=\"\"   height=\"\"   /></center>\n",
        "\n",
        "_Elaboración propia._\n",
        "\n",
        "***\n",
        "\n",
        "## Matriz inicial\n",
        "\n",
        "\\begin{align}\n",
        "    &m = \\text{número de imágenes} \\\\\n",
        "    &x = \\begin{bmatrix}\n",
        " x_{1\\,1}&  x_{1\\,2}&  x_{1\\,...}& x_{1\\,784}\\\\ \n",
        " x_{2\\,1}&  x_{2\\,2}&  x_{2\\,...}& x_{2\\,784}\\\\ \n",
        " x_{m-1\\,1}&  x_{m-1\\,2}&  x_{m-1\\,...}& x_{m-1\\,784}\\\\ \n",
        " x_{m\\,1}&  x_{m\\,2}&  x_{m\\,...}& x_{m\\,784}\n",
        "\\end{bmatrix}\n",
        "\\end{align}\n",
        "\n",
        "Se trabaja con la matriz transpuesta de $x$ ($x^{t}$), por lo que hay $m$ columnas con 784 filas.\n",
        "\n",
        "## Propagación hacia adelante (forward propagation)\n",
        "\n",
        "\\begin{align}\n",
        "    &A^{[0]} = \\text{capa de entrada (input layer)} \\\\\n",
        "    &Z^{[1]} = \\text{capa oculta (desactivada) (hidden layer)}\n",
        "\\end{align}\n",
        "\n",
        "Con la finalidad de obtener $Z^{[1]}$, se deberá aplicar un peso (weight, $w^{[1]}$) y un sesgo (bias, $b^{[1]}$), de manera que:\n",
        "\n",
        "\\begin{align}\n",
        "    &Z^{[1]} = w^{[1]}\\cdot A^{[0]} \\ + \\ b^{[1]} \\\\\n",
        "    &w^{[1]} = \\text{pesos de las conexiones de la primera capa (first layer connections weight)}\n",
        "\\end{align}\n",
        "\n",
        "Posteriormente, se aplica una \"función de activación\", en este caso **ReLU**<sup>3</sup>.\n",
        "\n",
        "\\begin{align}\n",
        "    &A^{[1]} \\ = \\ \\text{ReLU}(Z^{[1]})\n",
        "\\end{align}\n",
        "\n",
        "Y se pasa a la segunda capa:\n",
        "\n",
        "\\begin{align}\n",
        "    &Z^{[2]} = w^{[2]}\\cdot A^{[1]} \\ + \\ b^{[2]} \\\\\n",
        "    &w^{[2]} = \\text{pesos de las conexiones de la segunda capa (second layer connections weight)}\n",
        "\\end{align}\n",
        "\n",
        "Finalmente se aplicará la función de activación **softmax**<sup>3</sup>.\n",
        "\n",
        "\n",
        "\\begin{align}\n",
        "    &\\begin{bmatrix}\n",
        "6.66\\\\ \n",
        "2.5\\\\ \n",
        "1.3\n",
        "\\end{bmatrix}\\rightarrow \\, \\frac{e^{z_{i}}}{\\sum_{j \\ = \\ 1}^{K}e^{z_{j}}} \\,\\rightarrow \\, \\begin{bmatrix}\n",
        "0.95\\\\ \n",
        "0.027\\\\ \n",
        "0.023\n",
        "\\end{bmatrix}\n",
        "\\end{align}\n",
        "\n",
        "En el lado izquierdo se observan los valores de una capa de entrada (_input layer_), posterior a aplicada la función **softmax** estos valores se transforman en probabilidades, correspondiento a la probabilidad de que una imagen sea de cierta clase.\n",
        "\n",
        "\\begin{align}\n",
        "    &A^{[2]} \\ = \\ \\text{softmax}(Z^{[2]})\n",
        "\\end{align}\n",
        "\n",
        "## Propagación hacia atrás (backwards propagation)\n",
        "\n",
        "Se trata de ajustar los pesos según su contribución a la predicción inicial de la red. \n",
        "\n",
        "En este caso, se resta el _label_ que si le correspodía a la imagen procesada, por ejemplo, si el _label_ indicaba un 7, se hace esta resta a $A^{[2]}$, de la siguiente manera:\n",
        "\n",
        "\\begin{align}\n",
        "    &A^{[2]} - \\begin{bmatrix}\n",
        "0...\\\\ \n",
        "0...\\\\ \n",
        "0...\\\\ \n",
        "0...\\\\ \n",
        "0...\\\\ \n",
        "0...\\\\ \n",
        "0...\\\\ \n",
        "1...\\\\ \n",
        "0...\\\\ \n",
        "0...\n",
        "\\end{bmatrix}\n",
        "\\end{align}\n",
        "\n",
        "Nótese que el espacio en la matriz (ver tamaños reales abajo) que representa un 7 se le asigna un 1.\n",
        "\n",
        "El proceso busca determinar el error en la predicción, además se debe encontrar la contribución de $w^{[x]}$ y $b^{[x]}$, al error $dz^{[x]}$.\n",
        "\n",
        "\\begin{align}\n",
        "    &dz^{[2]} = A^{[2]} - \\gamma \\ \\text{(según label)} \\\\\n",
        "    &dw^{[2]} = \\frac{1}{m}dz^{[2]}A^{[1]t} \\\\\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "\n",
        "**AQUI** 9:20\n",
        "\n",
        "\n",
        "### Tamaños reales de las matrices\n",
        "\n",
        "\n",
        "\n",
        "***\n",
        "\n",
        "## Conceptos\n",
        "### Red neuronal<sup>1</sup>\n",
        "\n",
        "Se trata de un campo de estudio de la informática, que trata de \"reflejar\" el comportamiento del cerebro humano en el reconocimiento de patrones.<sup>[A]</sup>\n",
        "\n",
        "Una red se ve compuesta por:<sup>[A]</sup> \n",
        "\n",
        "- Capa de entrada (_input layer_).\n",
        "- Capas ocultas (_deep neural network_ ó _hidden layers_).\n",
        "- Capa de sálida (_output layer_).\n",
        "\n",
        "#### Funcionamiento\n",
        "\n",
        "- Cada nodo se conecta a otro, con un peso asociado.\n",
        "- Si la salida de un nodo se encuentra por encima de un umbral (_threshold_), se activa, mandando sus datos a la siguiente capa de la red.\n",
        "\n",
        "<center><img   src=\"https://i.imgur.com/kP4Dma1.jpg\"   width=\"\"   height=\"\"   /></center>\n",
        "\n",
        "_Elaboración propia._<sup>[B]</sup>\n",
        "\n",
        "### Capas<sup>2</sup>\n",
        "\n",
        "Las capas son el metacomponente principal de las redes neuronales. En cuanto a la **red oculta**, esta realiza distintas funciones que se encargan de la transformación (análisis) de los datos.<sup>[C]</sup>\n",
        "\n",
        "#### Neuronas (nodos)\n",
        "\n",
        "Las neuronas, son los componentes de las capas, comunicadas por conexiones, las cuales tienen un peso asignado.<sup>[D]</sup>\n",
        "\n",
        "\n",
        "### Funciones de activación<sup>3</sup> <sup>[E]</sup>\n",
        "\n",
        "Si se mantuviese la red de una forma lineal, recordando que, las neuronas de la red reciben una suma ponderada de de sus entradas (más el sesgo), dando el resultado a la siguiente capa, por lo que el resultado (en este caso) sería el equivalente a operar en una sola capa esos valores. \n",
        "\n",
        "La función de activación se colocará entre el receptor de la siguiente capa y la suma lineal de los componentes de la capa anterior.\n",
        "\n",
        "<center><img   src=\"https://i.imgur.com/niNM2kL.jpg\"   width=\"\"   height=\"\"   /></center>\n",
        "\n",
        "_Elaboración propia._\n",
        "\n",
        "Este ajuste servirá para el posterior proceso de propagación hacia atrás (backwards propagation), en el cual con ayuda de la derivada de la función de costo, se ajustarán los pesos de las conexiones dependiendo de como se ajuste a la predicción esperada.\n",
        "\n",
        "Algunas funciones son:\n",
        "\n",
        "- Sigmoide (logística): tomando en cuenta el [problema del desvanecimiento de gradiente](https://es.wikipedia.org/wiki/Problema_de_desvanecimiento_de_gradiente), se puede usar en clasificación binaria (perro o gato)\n",
        "- Tangente hiperbólica: comprende valores de -1 a 1, dando resultados más específicos que la Sigmoide.\n",
        "\n",
        "#### ReLU (unidad lineal rectificada) <sup>[E]</sup>\n",
        "\n",
        "\\begin{align}\n",
        "    &f(x)=\\max(0,x)\n",
        "\\end{align}\n",
        "\n",
        "Esta función no se encuentra acotada para números positivos, permitiendo un aprendizaje más rápido de la red.\n",
        "\n",
        "#### Softmax <sup>[F], [G]</sup>\n",
        "\n",
        "Es un tipo de función la cual soporta \"sistemas de clasificación multinomial\", siendo útil en el presente caso. \n",
        "\n",
        "Esta devuelve la distribución de probabilidad de cada una de las clases (posibles categorías, en este caso los dígitos).\n",
        "\n",
        "Posee la siguiente fórmula:\n",
        "\n",
        "\\begin{align}\n",
        "    &\\sigma (\\overrightarrow{z})_{i} \\ = \\ \\frac{e^{z_{i}}}{\\sum_{j \\ = \\ 1}^{K}e^{z_{j}}} \\\\\n",
        "    &\\overrightarrow{z} = \\text{vector de entrada (input vector).} \\\\\n",
        "    &z_{i} = \\text{todos los valores del vector de entrada (input vector values).} \\\\\n",
        "    &e^{z_{i}} = \\text{valor mayor que 0, pequeño si la entrada fue negativa, grande si la entrada lo fue.} \\\\\n",
        "    &\\text{(value greater than 0, if the input was negative the value is small, otherwise it is large.)} \\\\\n",
        "    &\\sum_{j \\ = \\ 1}^{K}e^{z_{j}} = \\text{normaliza el valor en el rango [0,1].} \\\\\n",
        "    &\\text{(normalizes the value in the range [0,1].)} \\\\\n",
        "    &K = \\text{número de clases (class number).}\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Referencias\n",
        "\n",
        "- <sup>[A]</sup> What are Neural Networks? (2021, August 03). Retrieved from https://www.ibm.com/cloud/learn/neural-networks\n",
        "- <sup>[B]</sup> ELEMENTOS BÁSICOS DE UNA RED NEURONAL ARTIFICIAL. (2007, October 01). Retrieved from https://advancedtech.wordpress.com/2007/08/31/elementos-basicos-de-una-red-neuronal-artificial\n",
        "- <sup>[C]</sup> Singh, H. (2021). Neural Network | Introduction to Neural Network | Neural Network for DL. Analytics Vidhya. Retrieved from https://www.analyticsvidhya.com/blog/2021/03/basics-of-neural-network\n",
        "- <sup>[D]</sup> Tech, R. (2021, July 07). Tu primera red neuronal en Python y Tensorflow. Youtube. Retrieved from https://www.youtube.com/watch?v=iX_on3VxZzk&list=PLZ8REt5zt2Pn0vfJjTAPaDVSACDvnuGiG\n",
        "- <sup>[E]</sup> Tech, R. (2022, May 25). Funciones de activación a detalle (Redes neuronales). Youtube. Retrieved from https://www.youtube.com/watch?v=_0wdproot34\n",
        "- <sup>[F]</sup> Las matemáticas del Machine Learning: Funciones de activación. (2021, June 25). Retrieved from https://empresas.blogthinkbig.com/las-matematicas-del-machine-learning-funciones-de-activacion\n",
        "- <sup>[G]</sup> Softmax Function. (2019, May 17). Retrieved from https://deepai.org/machine-learning-glossary-and-terms/softmax-layer\n"
      ],
      "metadata": {
        "id": "D1d6TiHBvpos"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Hello world!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8icSCDXrMPvF",
        "outputId": "2a401a54-c7e6-43ca-f75d-3c9b1718b81f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello world!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><h2 style=\"text-align: center;\">Licencia / License</h2><h3 style=\"text-align: center;\">Isaac-PM @ <a href=\"https://github.com/Isaac-PM\">https://github.com/Isaac-PM</a></h3><p style=\"text-align: center;\">Salvo que se indique lo contrario, esta obra tiene licencia <strong>Attribution-ShareAlike 4.0 International (CC BY-SA 4.0)</strong> @ <a href=\"https://creativecommons.org/licenses/by-sa/4.0/\">https://creativecommons.org/licenses/by-sa/4.0/</a></p><p style=\"text-align: center;\">&nbsp;</p><p style=\"text-align: center;\">Unless otherwise noted, this work is licensed <strong>Attribution-ShareAlike 4.0 International (CC BY-SA 4.0)</strong> @ <a href=\"https://creativecommons.org/licenses/by-sa/4.0/\">https://creativecommons.org/licenses/by-sa/4.0/</a>.</p><p style=\"text-align: center;\">&nbsp;</p><p style=\"text-align: center;\"><strong>Derechos reservados a los autores indicados en el campo de referencias.</strong></p></center>"
      ],
      "metadata": {
        "id": "pnYXJbgDesO0"
      }
    }
  ]
}